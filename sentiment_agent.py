import os
import json
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from datetime import datetime

# --- 1. CONFIGURATION ---
# Replace 'YOUR_API_KEY' with your actual Google Gemini API Key
# In production, use os.getenv("GEMINI_API_KEY")
API_KEY = os.getenv("GEMINI_API_KEY") or "AIzaSyCm9tKB61-DgSxeCZW7p5DHVqZXrIjLttU"

genai.configure(api_key=API_KEY)

class SentimentEngine:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    # --- 2. THE SCRAPER (Le Collecteur) ---
    def scrape_tunisian_news(self, ticker):
        """
        Attempts to scrape real news from IlBoursa.
        """
        print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Scraping news for {ticker}...")
        news_data = []
        
        # URL Strategy: Search for the ticker on IlBoursa
        # Note: In a real hackathon, if scraping fails due to anti-bot, we use the fallback.
        url = f"https://www.ilboursa.com/recherche?q={ticker}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Finding news articles (Tag logic specific to IlBoursa's structure)
                # We look for standard article links
                articles = soup.find_all('a', href=True, limit=5)
                
                for article in articles:
                    title = article.get_text(strip=True)
                    link = article['href']
                    
                    # Filter: Keep only titles that look like news (len > 20 chars)
                    if len(title) > 20 and ticker.lower() in title.lower():
                        news_data.append({
                            "source": "IlBoursa",
                            "title": title,
                            "url": f"https://www.ilboursa.com{link}" if link.startswith('/') else link
                        })
            else:
                print(f"‚ö†Ô∏è Website returned status {response.status_code}")

        except Exception as e:
            print(f"‚ö†Ô∏è Scraping error: {e}")

        # --- HACKATHON SAFETY NET (Fallback) ---
        # If scraping found nothing (common in demos), inject realistic mock data
        if not news_data:
            print("‚ö†Ô∏è No live news found (or blocked). Switching to SIMULATION MODE.")
            news_data = self.get_simulation_data(ticker)

        return news_data

    def get_simulation_data(self, ticker):
        """
        Provides realistic Tunisian data if scraping fails.
        """
        mock_db = {
            "SFBT": [
                {"source": "IlBoursa", "title": "SFBT: Revenus en hausse de 12% gr√¢ce √† la saison estivale"},
                {"source": "Facebook (Forum)", "title": "Rumeur: Le directeur financier de SFBT quitte le navire !"}
            ],
            "SAH": [
                {"source": "Tustex", "title": "SAH Lilas: Difficult√©s d'approvisionnement en cellulose"},
                {"source": "AfricanManager", "title": "SAH: Expansion vers le march√© Ivoirien confirm√©e"}
            ],
            "BIAT": [
                {"source": "IlBoursa", "title": "BIAT: B√©n√©fice net record pour l'ann√©e 2024"}
            ]
        }
        # Default generic news if ticker not in mock_db
        return mock_db.get(ticker, [{"source": "Bourse de Tunis", "title": f"Activit√© stable sur le titre {ticker} cette semaine."}])

    # --- 3. THE ANALYST (Le Cerveau Gemini) ---
    def analyze_with_gemini(self, ticker, news_items):
        print(f"üß† Sending {len(news_items)} articles to Gemini for analysis...")
        
        # Convert list to string for the prompt
        news_text = json.dumps(news_items, ensure_ascii=False)

        prompt = f"""
        You are an expert financial analyst for the Tunisian Stock Market (BVMT).
        Analyze the following news headlines for the company '{ticker}':
        {news_text}

        Your Goal: Produce a precise JSON assessment of the market sentiment.

        Rules for Analysis:
        1. **Bluff Detection**: If the source is social media (Facebook, Forums) or the title is clickbait/alarmist, set "is_bluff": true and "credibility": low (0.1 - 0.5). If source is IlBoursa/Tustex/Webmanager, credibility is high (0.8 - 1.0).
        2. **Impact Prediction**: Will this news move the price? (HIGH_VOLATILITY, MODERATE, LOW).
        3. **Sentiment**: Score from -1.0 (Very Negative) to 1.0 (Very Positive).
        4. **Language**: The input might be French or Arabic. Understand it, but output the 'why' field in French.

        OUTPUT FORMAT (Strict JSON, no markdown):
        {{
          "ticker": "{ticker}",
          "timestamp": "{datetime.now().isoformat()}",
          "global_sentiment_score": (float -1 to 1),
          "market_consensus": "STRONG_BUY" | "BUY" | "HOLD" | "SELL" | "STRONG_SELL",
          "news_analysis": [
            {{
              "title": "Title of the news",
              "source": "Source Name",
              "sentiment": (float -1 to 1),
              "credibility": (float 0 to 1),
              "is_bluff": (boolean),
              "predicted_impact": "HIGH_VOLATILITY" | "MODERATE" | "LOW",
              "why": "Brief explanation in French"
            }}
          ]
        }}
        """

        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            # Clean up JSON (remove ```json wrappers if Gemini adds them)
            clean_json = response.text.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_json)

        except Exception as e:
            print(f"‚ùå AI Error: {e}")
            return {"error": "Failed to analyze data"}

    # --- 4. THE PIPELINE (Run everything) ---
    def run_analysis(self, ticker):
        # Step 1: Get News (Scraped or Mock)
        news = self.scrape_tunisian_news(ticker)
        
        # Step 2: Analyze with AI
        final_json = self.analyze_with_gemini(ticker, news)
        
        return final_json

# --- EXECUTION (For testing) ---
if __name__ == "__main__":
    # Create the engine
    engine = SentimentEngine()
    
    # Test with SFBT (Should trigger the simulation if scraping fails)
    ticker_to_test = "SFBT"
    
    result = engine.run_analysis(ticker_to_test)
    
    # Print the FINAL JSON (This is what you send to Frontend)
    print("\n--- ‚úÖ FINAL JSON OUTPUT ---")
    print(json.dumps(result, indent=2, ensure_ascii=False))